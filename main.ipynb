{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/greca/anaconda3/lib/python3.8/site-packages (1.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/greca/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/greca/anaconda3/lib/python3.8/site-packages (from pandas) (2020.5)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/greca/anaconda3/lib/python3.8/site-packages (from pandas) (1.19.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/greca/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "#instala as bibliotecas necessárias para a execução\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando as bibliotecas que serão utilizadas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import warnings\n",
    "\n",
    "#serão ignoradas todas mensagens de warning (aviso) que aparecer no notebook\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abrindo o arquivo com os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_xls = pd.ExcelFile('data/teste_smarkio_Lbs.xls')\n",
    "dados_analise_ml = pd.read_excel(arquivo_xls, 'Análise_ML')\n",
    "dados_nlp = pd.read_excel(arquivo_xls, 'NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esses dados serão utilizados para resolver as atividades de 1 a 4.\n",
    "dados_analise_ml.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esses dados serão utilizados para resolver a atividade 5.\n",
    "dados_nlp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atividades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Análise exploratória dos dados utilizando estatística descritiva e inferencial,considerando uma, duas e/ou mais variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_class = classe que foi identificada pelo modelo\n",
    "#probabilidade = a probabilidade da classe que o modelo identificou\n",
    "#status = status da classificação de acordo com um especialista\n",
    "#true_class = classe verdadeira. Se for nula, assumir o valor do pred_class\n",
    "#se pred_class = true_class, então o modelo acertou a classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nós temos um total de 643 dados (também podemos chamar de linhas) e 4 colunas\n",
    "dados_analise_ml.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primeiramente iremos analisar os tipos das colunas que temos\n",
    "#'pred_class', 'probabilidade' e 'true_class' são colunas numéricas\n",
    "#'status' é do tipo object. Ou seja, se trata de uma coluna categórica\n",
    "dados_analise_ml.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_analise_ml.describe(include='all') #iremos incluir todas as colunas, até mesmo as categóricas (status)\n",
    "\n",
    "#a única coluna que possui valores nulos é a 'true_class', elas serão tratadas posteriormente\n",
    "#a coluna 'status' só possui dois valores possíveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise da coluna Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando a coluna 'status'\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.title('Análise da coluna Status')\n",
    "sns.countplot(dados_analise_ml['status'])\n",
    "fig.savefig('images/analise_status.png')\n",
    "\n",
    "#a coluna 'status' possui dois tipos de valores: approved e revision\n",
    "#approved é o valor que aparece com mais frequência (600), o que representa, aproximadamente, 93% do total\n",
    "#revision aparece somente 43 vezes\n",
    "#isso indica que temos mais dados aprovados do que os que precisam de uma revisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise da coluna True_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando a coluna 'true_class'\n",
    "dados_analise_ml_semnull = dados_analise_ml.copy() #armazenando uma cópia do nosso dataframe em outra variável\n",
    "\n",
    "#iremos fazer uma iteração em cada linha do nosso dataframe\n",
    "for index, linha in dados_analise_ml_semnull.iterrows():\n",
    "    \n",
    "    #verificamos se o valor para 'True_class' é nan (nulo)\n",
    "    if np.isnan(linha['True_class']):\n",
    "        \n",
    "        #se for nulo, iremos armazenar o valor do 'Pred_class' no seu lugar\n",
    "        dados_analise_ml_semnull.loc[index, 'True_class'] = linha['Pred_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando se ainda existe algum valor nulo no dataframe e se a inserção funcionou\n",
    "dados_analise_ml_semnull.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando se inserimos os valores certos\n",
    "\n",
    "#será criado um dataframe temporário para fazermos a comparação\n",
    "#teremos duas colunas 'True_class': a da esquerda é antes de ser tratada e a direita depois de ser tratada.\n",
    "#veremos que os valores foram inseridos corretamente\n",
    "temp_data = pd.concat([dados_analise_ml, dados_analise_ml_semnull['True_class']], axis=1)\n",
    "temp_data.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleta a variável temporária já que não usaremos ela para mais nada\n",
    "del temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usaremos só os dados onde o status é 'approved', pois foram confirmados e aprovados pelos cientistas\n",
    "dados_analise_ml_semnull_approved = dados_analise_ml_semnull[dados_analise_ml_semnull['status'] == 'approved'].copy()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.title('Análise da coluna True_class (Approved)')\n",
    "sns.countplot(y=dados_analise_ml_semnull_approved['True_class'], \n",
    "              orient='h',\n",
    "              order=dados_analise_ml_semnull_approved['True_class'].value_counts().index)\n",
    "fig.savefig('images/analise_true_class.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando o histograma da coluna 'True_class'\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.title('Histograma da coluna True_class (Approved)')\n",
    "sns.histplot(dados_analise_ml_semnull_approved['True_class'], kde=True)\n",
    "fig.savefig('images/histograma_true_class.png')\n",
    "\n",
    "#através do gráfico anterior (countplot) e do histograma mostrado abaixo\n",
    "#podemos perceber que as classes não são uniformemente distribuída, pois temos\n",
    "#mais classes 0, 2, 3, 4, 60, 74 e 77 do que as demais. Principalmente a 74.\n",
    "\n",
    "#podemos perceber também que os dados tem uma forma parecida com a de \n",
    "# dados que são assimétrico para a esquerda (positivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usaremos só os dados onde o status é 'revision'\n",
    "dados_analise_ml_semnull_revision = dados_analise_ml_semnull[dados_analise_ml_semnull['status'] == 'revision'].copy()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.title('Análise da coluna True_class (Revision)')\n",
    "sns.countplot(y=dados_analise_ml_semnull_revision['True_class'], \n",
    "              orient='h', \n",
    "              order=dados_analise_ml_semnull_revision['True_class'].value_counts().index)\n",
    "fig.savefig('images/analise_true_class_revision.png')\n",
    "\n",
    "#o compartamento dos dados com status 'revision' são parecidos com os dados\n",
    "#que possuem status 'approved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando o histograma da coluna 'True_class' (revision)\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.title('Histograma da coluna True_class (Revision)')\n",
    "sns.histplot(dados_analise_ml_semnull_revision['True_class'], kde=True)\n",
    "fig.savefig('images/histograma_true_class_revision.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temos um total de 69 classes diferentes (Approved)\n",
    "dados_analise_ml_semnull_approved['True_class'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temos um total de 73 classes diferentes (Total)\n",
    "dados_analise_ml_semnull['True_class'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando o boxplot da coluna, verificando se existe outlier\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.title('Análise boxplot True_class (Approved)')\n",
    "sns.boxplot(dados_analise_ml_semnull_approved['True_class'])\n",
    "fig.savefig('images/boxplot_true_class.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise da coluna Pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando a coluna 'pred_class'\n",
    "dados_analise_ml_semnull_approved = dados_analise_ml_semnull[dados_analise_ml_semnull['status'] == 'approved'].copy()\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.title('Análise da coluna Pred_class (Approved)')\n",
    "sns.countplot(y=dados_analise_ml_semnull_approved['Pred_class'],\n",
    "              orient='h',\n",
    "              order=dados_analise_ml_semnull_approved['Pred_class'].value_counts().index)\n",
    "fig.savefig('images/analise_pred_class.png')\n",
    "\n",
    "#as classes que mais apareceram na coluna 'Pred_class', que são os valores previstos pelo modelo\n",
    "#também são as que mais apareceram na coluna 'True_class', que são: 2, 3, 4, 60, 74 e 77.\n",
    "#o único que não aparece é a classe 0.\n",
    "#isso pode indicar que o valor 0, que está presente na coluna 'True_class', não representa uma classe\n",
    "#e pode ter sido um erro de digitação ou que o modelo tem dificuldades para reconhece-la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando o histograma da coluna 'Pred_class'\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.title('Histograma da coluna Pred_class (Approved)')\n",
    "sns.histplot(dados_analise_ml_semnull_approved['Pred_class'], kde=True)\n",
    "fig.savefig('images/histograma_pred_class.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantidade de classes diferentes previstas quando o status é approved\n",
    "dados_analise_ml_semnull_approved['Pred_class'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantidade de classes diferentes previstas no total\n",
    "dados_analise_ml_semnull['Pred_class'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando uma nova coluna no nosso dataframe para verificarmos se o algoritmo acertou a classe\n",
    "#se o classe real = prevista, colocamos o valor 1. Se não, o valor 0.\n",
    "dados_analise_ml_semnull_approved['Acertou'] = np.where(dados_analise_ml_semnull_approved['Pred_class'] == dados_analise_ml_semnull_approved['True_class'],\n",
    "                                                        1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_analise_ml_semnull_approved.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando as classes que tiveram o maior número de acertos\n",
    "dados_analise_ml_semnull_approved_acertou = dados_analise_ml_semnull_approved[dados_analise_ml_semnull_approved['Acertou'] == 1]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.title('Análise das colunas que tiveram um maior acerto')\n",
    "sns.countplot(y=dados_analise_ml_semnull_approved_acertou['True_class'],\n",
    "              orient='h',\n",
    "              order=dados_analise_ml_semnull_approved_acertou['True_class'].value_counts().index)\n",
    "fig.savefig('images/analise_classes_maior_acerto.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando as classes que tiveram o maior número de erros\n",
    "dados_analise_ml_semnull_approved_errou = dados_analise_ml_semnull_approved[dados_analise_ml_semnull_approved['Acertou'] == 0]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "plt.title('Análise das colunas que tiveram mais erros')\n",
    "sns.countplot(y=dados_analise_ml_semnull_approved_errou['True_class'],\n",
    "              orient='h',\n",
    "              order=dados_analise_ml_semnull_approved_errou['True_class'].value_counts().index)\n",
    "fig.savefig('images/analise_classes_maior_erro.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando o boxplot da coluna, verificando se existe outlier\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.title('Análise boxplot Pred_class (Approved)')\n",
    "sns.boxplot(dados_analise_ml_semnull_approved['Pred_class'])\n",
    "fig.savefig('images/boxplot_pred_class.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise da coluna Probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando a coluna probabilidade\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.title('Análise da coluna de probabilidade (Approved)')\n",
    "sns.histplot(dados_analise_ml_semnull_approved['probabilidade'], kde=True)\n",
    "fig.savefig('images/histograma_probabilidade.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analisando a coluna probabilidade quando acerta a classe\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.title('Análise da coluna de probabilidade quando acerta a classe')\n",
    "sns.histplot(dados_analise_ml_semnull_approved_acertou['probabilidade'], kde=True)\n",
    "fig.savefig('images/histograma_probabilidade_acertou.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlação entre as colunas\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(dados_analise_ml_semnull_approved.corr(), annot=True)\n",
    "fig.savefig('images/correlacao_colunas.png')\n",
    "\n",
    "#podemos ver que a coluna probabilidade tem uma correlação relativamente alta com a coluna 'acertou'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Calcule o desempenho do modelo de classificação utilizando pelo menos três métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Por se tratar de um problema de classificação, as métricas utilizadas são voltadas para\n",
    "#esse tipo de problema.\n",
    "#Um problema de classificação consiste em atribuir uma classe, dentre todas as possibilidades possíveis,\n",
    "#para uma determinada entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(dados_analise_ml_semnull['True_class'],\n",
    "                                    dados_analise_ml_semnull['Pred_class'])\n",
    "\n",
    "fig = plt.figure(figsize=(24, 18))\n",
    "sns.heatmap(confusion_matrix, cmap='Blues')\n",
    "plt.xlabel('Pred_class')\n",
    "plt.ylabel('True_class')\n",
    "\n",
    "#a confusion matrix (matriz de confusão) é boa para podermos visualizar os falsos negativos e positivos\n",
    "#das classes.\n",
    "#as áreas mais escuras quer dizer que teve uma maior ocorrência. \n",
    "#podemos observar que existem algumas áreas escuras fora da diagonal principal da matriz\n",
    "#nosso modelo confunde bastante o 0 com o 28 e o 44 com o 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = accuracy_score(dados_analise_ml_semnull_approved['True_class'],\n",
    "                       dados_analise_ml_semnull_approved['Pred_class'])\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula o score considerando os falsos negativo e verdadeiro positivo\n",
    "score = recall_score(dados_analise_ml_semnull_approved['True_class'],\n",
    "                     dados_analise_ml_semnull_approved['Pred_class'],\n",
    "                     average='weighted')\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula o score considerando os falsos positivos e verdadeiro positivo\n",
    "score = precision_score(dados_analise_ml_semnull_approved['True_class'],\n",
    "                        dados_analise_ml_semnull_approved['Pred_class'],\n",
    "                        average='weighted')\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Crie um classificador que tenha como output se os dados com status igual a revision estão corretos ou não (Sugestão : Técnica de cross-validation K-fold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dados_analise_ml_semnull_approved['Pred_class']\n",
    "y_train = dados_analise_ml_semnull_approved['True_class']\n",
    "\n",
    "#pega somente as linhas onde o status é 'revision'\n",
    "dados_analise_ml_revision = dados_analise_ml_semnull[dados_analise_ml_semnull['status'] == 'revision'].copy()\n",
    "\n",
    "X_test = dados_analise_ml_revision['Pred_class']\n",
    "y_test = dados_analise_ml_revision['True_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#para que possamos treinar o nosso classificador, primeiro devemos converter nossos\n",
    "#valores para um array de numpy.\n",
    "#isso deverá ser feito para que os nossos valores tenham a dimensão correta.\n",
    "#deverá ser feito tanto para o X_train quanto o X_test\n",
    "X_train_array = np.array(X_train)\n",
    "X_train_array = X_train_array.reshape(-1, 1)\n",
    "\n",
    "X_test_array = np.array(X_test)\n",
    "X_test_array = X_test_array.reshape(-1, 1)\n",
    "\n",
    "random_forest = RandomForestClassifier(random_state=23)\n",
    "\n",
    "#treinando o modelo com os dados que possuem status 'approved'\n",
    "random_forest.fit(X_train_array, y_train)\n",
    "\n",
    "#testando o modelo com os dados que possuem status 'revision'\n",
    "pred = random_forest.predict(X_test_array)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando um novo dataframe para podermos visualizar melhor a classe prevista e a classe real\n",
    "dataframe_comparacao = pd.DataFrame({\n",
    "    'Classe_real': y_test,\n",
    "    'Classe_prevista': pred\n",
    "})\n",
    "\n",
    "#criando uma nova coluna no nosso dataframe para verificarmos se o algoritmo acertou a classe\n",
    "#se o classe real = prevista, colocamos o valor 1. Se não, o valor 0.\n",
    "dataframe_comparacao['Acertou'] = np.where(dataframe_comparacao['Classe_real'] == dataframe_comparacao['Classe_prevista'],\n",
    "                                          1, 0)\n",
    "\n",
    "dataframe_comparacao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#podemos ver que o modelo acertou uma grande quantidade de dados que estavam com status 'revision'\n",
    "print(\"Total de acertos: \" + str(dataframe_comparacao['Acertou'].sum()) + \" de \" + str(dataframe_comparacao.shape[0]))\n",
    "print(\"Porcentagem: \" + str(dataframe_comparacao['Acertou'].sum()/dataframe_comparacao.shape[0]))\n",
    "\n",
    "#iremos obter um score de, aproximadamente, 0.814 no classificador criado\n",
    "#isso poderia ser melhorado usando outros algoritmos mais eficientes (por exemplo: XGBoost e LightGBM)\n",
    "#e também usando técnicas para melhorar os hiper parâmetros do modelo (GridSearch, RandomSearch ou Optuna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Compare três métricas de avaliação aplicadas ao modelo e descreva sobre a diferença."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula o score utilizando verdadeiro positivo, verdadeiro negativo, falso positivo e falso negativo\n",
    "#fórmula= (vp + vn) / (vp + vn + fp + fn)\n",
    "#vp= verdadeiro positivo\n",
    "#vn= verdadeiro negativo\n",
    "#fp= falso positivo\n",
    "#fn= falso negativo\n",
    "\n",
    "score = accuracy_score(dataframe_comparacao['Classe_real'],\n",
    "                       dataframe_comparacao['Classe_prevista'])\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula o score utilizando verdadeiro positivo e falso negativo\n",
    "#fórmula= vp / (vp + fn)\n",
    "#vp= verdadeiro positivo\n",
    "#fn= falso negativo\n",
    "\n",
    "#estamos usando o 'weighted' para calcular o score de cada classe e aplicando um peso sobre ele\n",
    "#no nosso caso, como as classes não estão distribuídas de forma uniforme é necessário usar o peso\n",
    "#para que o score fique balanceado\n",
    "score = recall_score(dataframe_comparacao['Classe_real'],\n",
    "                     dataframe_comparacao['Classe_prevista'],\n",
    "                     average='weighted')\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizando o 'macro' só iremos calcular o score de classe e não aplicamos um peso sobre ela\n",
    "#podemos notar a diferença de um score para o outro\n",
    "score = recall_score(dataframe_comparacao['Classe_real'],\n",
    "                     dataframe_comparacao['Classe_prevista'],\n",
    "                     average='macro')\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calcula o score utilizando verdadeiro positivo e falso positivo\n",
    "#fórmula= vp / (vp + fp)\n",
    "#vp= verdadeiro positivo\n",
    "#fp= falso positivo\n",
    "\n",
    "#estamos usando o 'weighted' para calcular o score de cada classe e aplicando um peso sobre ele\n",
    "#no nosso caso, como as classes não estão distribuídas de forma uniforme é necessário usar o peso\n",
    "#para que o score fique balanceado\n",
    "score = precision_score(dataframe_comparacao['Classe_real'],\n",
    "                        dataframe_comparacao['Classe_prevista'],\n",
    "                        average='weighted')\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizando o 'macro' só iremos calcular o score de classe e não aplicamos um peso sobre ela\n",
    "#podemos notar a diferença de um score para o outro\n",
    "score = precision_score(dataframe_comparacao['Classe_real'],\n",
    "                     dataframe_comparacao['Classe_prevista'],\n",
    "                     average='macro')\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Crie um classificador, a partir da segunda aba - NLP do arquivo de dados, quepermita identificar qual trecho de música corresponde às respectivas artistas listadas (Sugestão: Naive Bayes Classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temos 518 dados e duas colunas\n",
    "#colunas: letra e artista\n",
    "#letra: trecho da música\n",
    "#artista: cantora referente ao trecho da música (letra)\n",
    "dados_nlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verificando se existe linhas com nulo\n",
    "dados_nlp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.title('Análise dos Artistas')\n",
    "sns.countplot(dados_nlp['artista'])\n",
    "\n",
    "#Temos dois artistas: Beyoncé e Rihanna\n",
    "#Beyoncé aparece 274 vezes, aproximadamente 53% \n",
    "#Rihanna aparece 244 vezes, aproximadamente 47%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Criação do classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separando os dados em X(features) e y(target)\n",
    "\n",
    "X = dados_nlp['letra']\n",
    "y = dados_nlp['artista']\n",
    "\n",
    "#separando os dados para treinamento do classificador e para testá-lo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=23)\n",
    "\n",
    "#para a extração de features será utilizado o método bags of words\n",
    "#o que esse método faz é associar um número inteiro para cada palavra\n",
    "#e depois armazenará a quantidade de vezes que essa palavra apareceu.\n",
    "#a função CountVectorizer já realiza todo esse processo de pré processamento e \n",
    "#tokenização (associar ao número inteiro) das palavras.\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_tratado = cv.fit_transform(X_train)\n",
    "X_test_tratado = cv.transform(X_test)\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "#treinando o classificador\n",
    "naive_bayes.fit(X_train_tratado, y_train)\n",
    "\n",
    "#testando o classificador com os dados de teste\n",
    "pred = naive_bayes.predict(X_test_tratado)\n",
    "np.mean(pred == y_test)\n",
    "\n",
    "#iremos obter um score de 0.75 no classificador criado\n",
    "#isso poderia ser melhorado usando outros algoritmos mais eficientes (por exemplo o SVM), já que o Naive Bayes é mais básico\n",
    "#e também usando técnicas para melhorar os hiper parâmetros do modelo (GridSearch, RandomSearch ou Optuna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
